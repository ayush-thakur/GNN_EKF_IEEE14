{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from wls se files started\n",
      "reading from wls se files done\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "reading from wls se files started\n",
      "reading from wls se files done\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "reading from wls se files started\n",
      "reading from wls se files done\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "from csv import *\n",
    "\n",
    "\n",
    "def generate_datasets():\n",
    "    data_dir = os.path.abspath(\"./data_from_wls_se_solver\")\n",
    "\n",
    "    generate_dataset(datasetType=\"train\", data_dir=data_dir, should_connect_node_to_second_neighbours=True)\n",
    "\n",
    "    generate_dataset(datasetType=\"test\", data_dir=data_dir, should_connect_node_to_second_neighbours=True)\n",
    "\n",
    "    generate_dataset(datasetType=\"validation\", data_dir=data_dir, should_connect_node_to_second_neighbours=True)\n",
    "\n",
    "\n",
    "def generate_dataset(datasetType, data_dir, should_connect_node_to_second_neighbours=True):\n",
    "    \"\"\"\n",
    "\n",
    "    :param datasetType: train, validation or test set\n",
    "    :param should_connect_node_to_second_neighbours: if true, adds direct variable-to-variable node connections, i.e.\n",
    "           creates the augmented factor graph from the factor graph\n",
    "\n",
    "    local variables:\n",
    "    numMeasurements - size of measurement vector, number of factor nodes, it is two times larger than the number of measurement phasors\n",
    "    numGraphs - number of samples in the dataset\n",
    "    \"\"\"\n",
    "    print(\"reading from wls se files started\")\n",
    "    numGraphs, numVariableNodes, numMeasurements, estimate_rows, jacobian_rows, measurement_rows, variance_rows, covariance_rows = read_from_wls_se_files(\n",
    "        datasetType, data_dir)\n",
    "    print(\"reading from wls se files done\")\n",
    "\n",
    "    jsonFilePath = open_json_dataset_file(datasetType)\n",
    "\n",
    "    encoded_variable_node_indices = encode_variable_node_indices(numVariableNodes)\n",
    "\n",
    "    jacobRowCount = 0\n",
    "    for iGraph in range(numGraphs):\n",
    "        if iGraph % 10 == 0:\n",
    "            print(iGraph)\n",
    "\n",
    "        G = nx.DiGraph()  # undirected graph\n",
    "\n",
    "        measurement_row = measurement_rows[iGraph]\n",
    "        variance_row = variance_rows[iGraph]\n",
    "        estimate_row = estimate_rows[iGraph]\n",
    "        covariance_row_temp = covariance_rows[iGraph]\n",
    "        # since the number of covariances is two times smaller than the number of factor nodes, because of easier implementation\n",
    "        # we double the number of covariances in the covariance list.\n",
    "        covariance_row = []\n",
    "        for x in covariance_row_temp:\n",
    "            covariance_row.append(x)\n",
    "            covariance_row.append(x)\n",
    "\n",
    "        if len(measurement_row) != numMeasurements:\n",
    "            print(\"ERROR: len(measurement_row) != numMeasurements\")\n",
    "            return\n",
    "        if len(variance_row) != numMeasurements:\n",
    "            print(\"ERROR: len(variance_row) != numMeasurements\")\n",
    "            return\n",
    "        if len(covariance_row) != numMeasurements:\n",
    "            print(\"ERROR: len(covariance_row) != numMeasurements\")\n",
    "            return\n",
    "        if len(estimate_row) != numVariableNodes:\n",
    "            print(\"ERROR: len(estimate_row) != numVariableNodes\")\n",
    "            return\n",
    "\n",
    "        add_variable_nodes(G, estimate_row, encoded_variable_node_indices, numVariableNodes, iGraph)\n",
    "\n",
    "        add_factor_nodes(G, covariance_row, measurement_row, numMeasurements, numVariableNodes, variance_row)\n",
    "\n",
    "        if G.number_of_nodes() != numMeasurements + numVariableNodes:\n",
    "            print(\"ERROR: G.number_of_nodes() != numMeasurements + numVariableNodes\")\n",
    "            return\n",
    "\n",
    "        jacobRowCount = add_graph_edges(G, jacobRowCount, jacobian_rows, numMeasurements, numVariableNodes)\n",
    "\n",
    "        connect_nodes_to_second_neighbours(G, numVariableNodes, should_connect_node_to_second_neighbours)\n",
    "\n",
    "        parced_graph = json_graph.node_link_data(G)\n",
    "        with open(jsonFilePath, 'a') as f:\n",
    "            json.dump(parced_graph, f)\n",
    "            f.write(\",\")\n",
    "\n",
    "    with open(jsonFilePath, mode=\"r+\") as file:\n",
    "        file.seek(os.stat(jsonFilePath).st_size - 1)  # override the last comma in the file\n",
    "        file.write(\"]\")\n",
    "\n",
    "\n",
    "def encode_variable_node_indices(numVariableNodes):\n",
    "    data = pd.DataFrame(\n",
    "        {'nodeIdx': [i for i in range(numVariableNodes)]})\n",
    "    encoder = ce.BaseNEncoder(cols=['nodeIdx'], return_df=False, base=2)\n",
    "    encoded_variable_node_indices = encoder.fit_transform(data)\n",
    "    return encoded_variable_node_indices\n",
    "\n",
    "\n",
    "def open_json_dataset_file(datasetType):\n",
    "    if datasetType == \"test\":\n",
    "        with open(os.path.join('data/test', 'data.json'), 'w') as json_file:\n",
    "            json_file.write(\"[\")\n",
    "            jsonFilePath = os.path.join('data/test', 'data.json')\n",
    "    elif datasetType == \"validation\":\n",
    "        with open(os.path.join('data/validation', 'data.json'), 'w') as json_file:\n",
    "            json_file.write(\"[\")\n",
    "            jsonFilePath = os.path.join('data/validation', 'data.json')\n",
    "    else:\n",
    "        jsonFilePath = os.path.join('data/train', 'data.json')\n",
    "        with open(os.path.join('data/train', 'data.json'), 'w') as json_file:\n",
    "            json_file.write(\"[\")\n",
    "    return jsonFilePath\n",
    "\n",
    "\n",
    "def add_graph_edges(G, jacobRowCount, jacobian_rows, numMeasurements, numVariableNodes):\n",
    "    # jacobRowCount = 0  # sluzi da bismo prolazili po svim grafovima u okviru jacobian matrice\n",
    "    # variable nodes: 0..numVariables-1\n",
    "    # factor nodes: numVariables..numVariables + numMeasurements - 1\n",
    "    for iMeasurement in range(numMeasurements):\n",
    "        jacobRow = jacobian_rows[jacobRowCount]\n",
    "        jacobRowCount += 1\n",
    "\n",
    "        factorNodeIndex = iMeasurement + numVariableNodes\n",
    "        for iVariable in range(numVariableNodes):\n",
    "            if abs(float(jacobRow[iVariable])) > 0.0001:\n",
    "                G.add_edge(str(factorNodeIndex), str(iVariable))\n",
    "                # IGNNITION received as input an undirected graph, even though it only\n",
    "                # supports (at the moment) directed graphs -> therefore we must double the number of edges.\n",
    "                G.add_edge(str(iVariable), str(factorNodeIndex))\n",
    "    return jacobRowCount\n",
    "\n",
    "\n",
    "def add_variable_nodes(G, estimate_row, encoded_variable_node_indices, numVariableNodes, iGraph):\n",
    "    for iVar in range(numVariableNodes):\n",
    "        index_encoding = encoded_variable_node_indices[iVar].tolist()\n",
    "        G.add_node(str(iVar), entity='variableNode', voltage=estimate_row[iVar], index_encoding=index_encoding)\n",
    "        # self loop:\n",
    "        G.add_edge(str(iVar), str(iVar))\n",
    "\n",
    "\n",
    "def add_factor_nodes(G, covariance_row, measurement_row, numMeasurements, numVariableNodes, variance_row):\n",
    "    for iMeasur in range(numMeasurements):\n",
    "        meas = float(measurement_row[iMeasur])\n",
    "        var = math.log10(float(variance_row[iMeasur])) / 10.0\n",
    "        covar = float(covariance_row[iMeasur]) * 10.0\n",
    "        factorNodeIndex = iMeasur + numVariableNodes\n",
    "        G.add_node(str(factorNodeIndex), entity='factorNode', measurement=meas, variance=var, covariance=covar)\n",
    "        # self loop:\n",
    "        G.add_edge(str(factorNodeIndex), str(factorNodeIndex))\n",
    "\n",
    "\n",
    "def connect_nodes_to_second_neighbours(G, numVariableNodes, should_connect_node_to_second_neighbours):\n",
    "    if should_connect_node_to_second_neighbours:\n",
    "        for iVariable in range(numVariableNodes):\n",
    "            connect_node_to_second_neighbours(G, str(iVariable))\n",
    "\n",
    "\n",
    "def get_second_neighbors(G, node):\n",
    "    return [nodeId for nodeId, pathLength in nx.single_source_shortest_path_length(G, node, cutoff=2).items() if\n",
    "            pathLength == 2]\n",
    "\n",
    "\n",
    "def connect_node_to_second_neighbours(G, variableNodeId):\n",
    "    for neighrbVariableNodeId in get_second_neighbors(G, variableNodeId):\n",
    "        G.add_edge(variableNodeId, neighrbVariableNodeId)\n",
    "\n",
    "\n",
    "def read_from_wls_se_files(datasetType, data_dir):\n",
    "    if datasetType == \"test\":\n",
    "        path = str(data_dir) + \"/Test_Estimate.csv\"\n",
    "    elif datasetType == \"validation\":\n",
    "        path = str(data_dir) + \"/Validation_Estimate.csv\"\n",
    "    else:\n",
    "        path = str(data_dir) + \"/Training_Estimate.csv\"\n",
    "    with open(path, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        estimate_rows = list(csv_reader)\n",
    "\n",
    "    if datasetType == \"test\":\n",
    "        path = str(data_dir) + \"/Test_Jacobian.csv\"\n",
    "    elif datasetType == \"validation\":\n",
    "        path = str(data_dir) + \"/Validation_Jacobian.csv\"\n",
    "    else:\n",
    "        path = str(data_dir) + \"/Training_Jacobian.csv\"\n",
    "    with open(path, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        jacobian_rows = []\n",
    "        for i, line in enumerate(csv_reader):\n",
    "            jacobian_row = [1 if abs(float(element)) > 0.0001 else 0 for element in line]\n",
    "            jacobian_rows.append(jacobian_row)\n",
    "\n",
    "    if datasetType == \"test\":\n",
    "        path = str(data_dir) + \"/Test_Measurement.csv\"\n",
    "    elif datasetType == \"validation\":\n",
    "        path = str(data_dir) + \"/Validation_Measurement.csv\"\n",
    "    else:\n",
    "        path = str(data_dir) + \"/Training_Measurement.csv\"\n",
    "    with open(path, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        measurement_rows = list(csv_reader)\n",
    "\n",
    "    if datasetType == \"test\":\n",
    "        path = str(data_dir) + \"/Test_Variance.csv\"\n",
    "    elif datasetType == \"validation\":\n",
    "        path = str(data_dir) + \"/Validation_Variance.csv\"\n",
    "    else:\n",
    "        path = str(data_dir) + \"/Training_Variance.csv\"\n",
    "    with open(path, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        variance_rows = list(csv_reader)\n",
    "\n",
    "    if datasetType == \"test\":\n",
    "        path = str(data_dir) + \"/Test_Covariance.csv\"\n",
    "    elif datasetType == \"validation\":\n",
    "        path = str(data_dir) + \"/Validation_Covariance.csv\"\n",
    "    else:\n",
    "        path = str(data_dir) + \"/Training_Covariance.csv\"\n",
    "    with open(path, 'r') as read_obj:\n",
    "        csv_reader = reader(read_obj)\n",
    "        covariance_rows = list(csv_reader)\n",
    "\n",
    "    if datasetType == \"test\":\n",
    "        path = str(data_dir) + \"/Test_NumMeasurements.txt\"\n",
    "    elif datasetType == \"validation\":\n",
    "        path = str(data_dir) + \"/Validation_NumMeasurements.txt\"\n",
    "    else:\n",
    "        path = str(data_dir) + \"/Training_NumMeasurements.txt\"\n",
    "    file1 = open(path, 'r')\n",
    "    lines = file1.readlines()\n",
    "    numLinePMeasurements = int(lines[0])\n",
    "    numPInjMeasurements = int(lines[1])\n",
    "    numThetaMeasurements = int(lines[2])\n",
    "\n",
    "    numGraphs = int(lines[3])\n",
    "    numVariableNodes = int(lines[4]) * 2  # Vream and Vimag\n",
    "    numMeasurements = numLinePMeasurements + numPInjMeasurements + numThetaMeasurements\n",
    "\n",
    "    if len(jacobian_rows) != numGraphs * numMeasurements:\n",
    "        print(\n",
    "            \"ERROR: len(jacobian_rows) != numGraphs * (numLinePMeasurements + numPInjMeasurements + numThetaMeasurements)\")\n",
    "        print(\"len(jacobian_rows) \", len(jacobian_rows))\n",
    "        print(\"numGraphs \", numGraphs)\n",
    "        print(\"numLinePMeasurements + numPInjMeasurements + numThetaMeasurements  \",\n",
    "              numLinePMeasurements + numPInjMeasurements + numThetaMeasurements)\n",
    "        return\n",
    "\n",
    "    if len(jacobian_rows[0]) != numVariableNodes:\n",
    "        print(\"ERROR: len(jacobian_rows[0]) != numVariableNodes\")\n",
    "        return\n",
    "\n",
    "    if len(estimate_rows) != numGraphs:\n",
    "        print(\"ERROR: len(estimate_rows) != numGraphs\")\n",
    "        return\n",
    "\n",
    "    if len(estimate_rows[0]) != numVariableNodes:\n",
    "        print(\"ERROR: len(estimate_rows[0]) != numVariableNodes\")\n",
    "        return\n",
    "\n",
    "    if len(measurement_rows) != numGraphs:\n",
    "        print(\"ERROR: len(measurement_rows) != numGraphs\")\n",
    "        return\n",
    "\n",
    "    if len(measurement_rows[0]) != numMeasurements:\n",
    "        print(\"ERROR: len(measurement_rows[0]) != numMeasurements\")\n",
    "        return\n",
    "\n",
    "    if len(variance_rows) != numGraphs:\n",
    "        print(\"ERROR: len(variance_rows) != numGraphs\")\n",
    "        return\n",
    "\n",
    "    if len(variance_rows[0]) != numMeasurements:\n",
    "        print(\"ERROR: len(variance_rows[0]) != numMeasurements\")\n",
    "        return\n",
    "\n",
    "    if len(covariance_rows) != numGraphs:\n",
    "        print(\"ERROR: len(covariance_rows) != numGraphs\")\n",
    "        return\n",
    "\n",
    "    # there is one covariance value per measurement phasor, while numMeasurements is the size of\n",
    "    # measurement vector, i.e. it is two times larger than the number of measurement phasors\n",
    "    if 2 * len(covariance_rows[0]) != numMeasurements:\n",
    "        print(\"ERROR: len(variance_rows[0]) != numMeasurements\")\n",
    "        return\n",
    "\n",
    "    return numGraphs, numVariableNodes, numMeasurements, estimate_rows, jacobian_rows, measurement_rows, variance_rows, covariance_rows\n",
    "\n",
    "\n",
    "generate_datasets()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
